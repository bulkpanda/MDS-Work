{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from variableUtils import *\n",
    "import variableUtils\n",
    "from Utils import *\n",
    "# from ClassUtils import *\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'FullSpreadsheets\\CAF+v0.1_September+29,+2024_20.45\\CAF v0.1_September 29, 2024_20.45.csv'\n",
    "folder, filename, ext = getFolderandFileName(filepath)\n",
    "df = pd.read_csv(filepath)\n",
    "# print(df[colComments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueComments = df[colComments].unique()\n",
    "uniqueComments2 = df['Further comments'].unique()\n",
    "# remove nan\n",
    "uniqueComments = uniqueComments[~pd.isnull(uniqueComments)]\n",
    "uniqueComments2 = uniqueComments2[~pd.isnull(uniqueComments2)]\n",
    "comments = uniqueComments.tolist() + uniqueComments2.tolist()\n",
    "print(len(comments))\n",
    "for comment in comments:\n",
    "    print(comment)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# 1. Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# 2. Extract phrases (n-grams)\n",
    "def extract_phrases(comments, n=2):\n",
    "    all_phrases = []\n",
    "    for comment in comments:\n",
    "        tokens = preprocess_text(comment)\n",
    "        phrases = list(ngrams(tokens, n))\n",
    "        all_phrases.extend(phrases)\n",
    "    return all_phrases\n",
    "\n",
    "# 3. Get most common phrases\n",
    "def get_common_phrases(comments, n=2):\n",
    "    phrases = extract_phrases(comments, n)\n",
    "    phrase_counter = Counter(phrases)\n",
    "    return phrase_counter.most_common(10)  # Get top 10 most common phrases\n",
    "\n",
    "# Example usage:\n",
    "for comment in comments[:10]:\n",
    "    print(comment)\n",
    "    tokens = preprocess_text(comment)\n",
    "    print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "common_phrases = get_common_phrases(comments, 2)\n",
    "print(\"Common Phrases:\", common_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "# from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "# 1. Combine all comments into one large text corpus\n",
    "corpus = ' '.join(comments)\n",
    "\n",
    "# 2. Initialize KeyBERT model\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "# 3. Extract key phrases from the entire corpus\n",
    "key_phrases = kw_model.extract_keywords(corpus, keyphrase_ngram_range=(2, 5), top_n=20, stop_words='english')\n",
    "\n",
    "print(\"Key Phrases:\", key_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Sample data: list of comments\n",
    "# documents = [\"Comment one text goes here\", \"Comment two text goes here\", ...]\n",
    "\n",
    "# Step 1: Preprocessing\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([word for word in doc.lower().split() if word not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in comments]\n",
    "\n",
    "# Step 2: Preparing Document-Term Matrix\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "corpus = [dictionary.doc2bow(text) for text in doc_clean]\n",
    "\n",
    "# Step 3: LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=50)\n",
    "\n",
    "# Step 4: Results\n",
    "topics = ldamodel.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "# Load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Example comments\n",
    "# comments = [\n",
    "#     \"I love the new update, itâ€™s sleek and easy to use.\",\n",
    "#     \"The new update is amazing, very user-friendly.\",\n",
    "#     \"Not happy with the service, it was very slow.\",\n",
    "#     \"The service was disappointing, took too long to respond.\",\n",
    "#     \"Great job on the latest event, very well organized!\",\n",
    "#     \"The event was fantastic, had a great time!\"\n",
    "# ]\n",
    "\n",
    "# Step 1: Preprocess comments\n",
    "def preprocess(text):\n",
    "    doc = nlp(text.lower())\n",
    "    result = ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "    return result\n",
    "\n",
    "preprocessed_comments = [preprocess(comment) for comment in comments]\n",
    "\n",
    "# Step 2: Convert text to vectors\n",
    "embeddings = sbert_model.encode(preprocessed_comments)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "num_clusters = 50\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(embeddings)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Organize comments by clusters\n",
    "clustered_comments = {i: [] for i in range(num_clusters)}\n",
    "for comment, label in zip(comments, labels):\n",
    "    clustered_comments[label].append(comment)\n",
    "\n",
    "# Print all comments for each cluster\n",
    "for cluster, cluster_comments in clustered_comments.items():\n",
    "    print(f\"Cluster {cluster + 1}:\")\n",
    "    for comment in cluster_comments:\n",
    "        print(f\" - {comment}\")\n",
    "    print()  # Add an empty line for better separation\n",
    "\n",
    "# save the clusters and the comments in a file\n",
    "with open(f'{folder}/clusters.txt', 'w') as f:\n",
    "    for cluster, cluster_comments in clustered_comments.items():\n",
    "        f.write(f\"Cluster {cluster + 1}:\\n\")\n",
    "        for comment in cluster_comments:\n",
    "            f.write(f\" - {comment}\\n\")\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(n, tot):\n",
    "    return (tot - n) / tot * 100\n",
    "tot = [7, 4, 3, 5]\n",
    "\n",
    "x=[1, 0, 0, 1]\n",
    "# for i in range(4):\n",
    "#     print(calc(x[i], tot[i]))\n",
    "xlist = [[1, 0, 0, 1], [0, 0, 0, 0], [2, 0, 0, 2], [0, 0, 0, 1], [1, 0, 0, 1], [0, 0, 0, 1], [1, 0, 0, 1], [3, 0, 0, 0]]\n",
    "for x in xlist:\n",
    "    print(calc(x[0], 7), calc(x[1], 4), calc(x[2], 3), calc(x[3], 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To get words that occur togther and are interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# change this to read in your data\n",
    "finder = BigramCollocationFinder.from_words(\n",
    "    nltk.corpus.genesis.words('english-web.txt'))\n",
    "\n",
    "# only bigrams that appear 3+ times\n",
    "finder.apply_freq_filter(3)\n",
    "\n",
    "# return the 10 n-grams with the highest PMI\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
